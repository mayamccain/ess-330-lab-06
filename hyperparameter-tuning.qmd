---
title: "hyperparameter-tuning"
author: "Maya McCain"
format: html
editor: visual
---

# Data Import/Tidy/Transform

## Library Loading

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

## Data Ingest

```{r}
camels <- map(local_files, read_delim, show_col_types = FALSE)
camels <- power_full_join(camels ,by = 'gauge_id')
```

## Data Cleaning

```{r}
glimpse(camels)
skimr::skim(camels)
visdat::vis_dat(camels)

camels <- camels %>% 
  drop_na()
glimpse(camels)
visdat::vis_dat(camels)
```

# Data Splitting

## Initial Split

```{r}
set.seed(123)
camels <- camels |> 
  mutate(logQmean = log(q_mean))
```

## Testing/Training

```{r}
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)
```

# Feature Engineering

## Proper Recipe

```{r}
rec <- recipe(logQmean ~ slope_mean + runoff_ratio, data = camels_train) %>%
  step_naomit(all_predictors(), all_outcomes()) %>%
  step_log(slope_mean, runoff_ratio, offset = 1) %>%
  step_interact(terms = ~ slope_mean:runoff_ratio)
```

# Data Resampling and Model Testing

## Cross Validation Dataset (k-folds)

```{r}
camels_cv <- vfold_cv(camels_train, v = 10)
```

## Define Three Regression Models
```{r}
# Random Forest Model
rf_model <- rand_forest() |>
  set_engine("randomForest") |>
  set_mode("regression")

# XGBoost Model
xgb_model <- boost_tree() |>
  set_engine('xgboost') |>
  set_mode("regression")

# Neural Network Model
nn_model <- mlp(hidden = 10) |>
  set_engine('nnet') |>
  set_mode("regression")
```

## Workflow Set/Map/Autoplot
```{r}
wf <- workflow_set(list(rec), list(rf_model, xgb_model, nn_model)) |>
  workflow_map(resamples = camels_cv)

autoplot(wf)
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

## Model Selection with Justification
The neural network model is the best model for looking at this relationship. As shown in the autoplot, the neural network model has an r-squared of about 0.94, which shows a higher correlation between mean slope and runoff ratio. The lower root mean squared error of about 0.255 shows that there is a higher correlation between the variables because there is less error.

The neural network model is based on the human brain in order to make predictions and identify patterns. It used the 'nnet' engine and the regression mode. This model likely performs well for stream flow prediction using slope and runoff ratio because it may be a nonlinear relationship, and the neural network model works well with these relationships. The neural network is also good at generalization and is good at adapting to different data. These qualities are useful when studying this relationship. 